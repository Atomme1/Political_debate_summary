{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49a65b9d",
   "metadata": {},
   "source": [
    "**Install packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7351c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install vaderSentiment-fr\n",
    "#!pip install python-Levenshtein\n",
    "#!pip install spacy\n",
    "#!python -m spacy download fr_core_news_sm\n",
    "#!python -m spacy download fr_core_news_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238b8d11",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4abb7ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment_fr.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences \n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221da056",
   "metadata": {},
   "source": [
    "***Normalization of the data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9986bf60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape : (12,)----------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple cherche à acheter une start-up anglaise ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Les voitures autonomes déplacent la responsabi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>San Francisco envisage d'interdire les robots ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Londres est une grande ville du Royaume-Uni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L’Italie choisit ArcelorMittal pour reprendre ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Apple lance HomePod parce qu'il se sent menacé...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>La France ne devrait pas manquer d'électricité...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nouvelles attaques de Trump contre le maire de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Où es-tu ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Qui est le président de la France ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Où est la capitale des États-Unis ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Quand est né Barack Obama ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentences\n",
       "0   Apple cherche à acheter une start-up anglaise ...\n",
       "1   Les voitures autonomes déplacent la responsabi...\n",
       "2   San Francisco envisage d'interdire les robots ...\n",
       "3         Londres est une grande ville du Royaume-Uni\n",
       "4   L’Italie choisit ArcelorMittal pour reprendre ...\n",
       "5   Apple lance HomePod parce qu'il se sent menacé...\n",
       "6   La France ne devrait pas manquer d'électricité...\n",
       "7   Nouvelles attaques de Trump contre le maire de...\n",
       "8                                          Où es-tu ?\n",
       "9                 Qui est le président de la France ?\n",
       "10                Où est la capitale des États-Unis ?\n",
       "11                        Quand est né Barack Obama ?"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = np.array(sentences)\n",
    "print(\"Data shape : {0}\".format(np.shape(sentences)).ljust(40,'-'))\n",
    "\n",
    "columns = ['sentences']\n",
    "df = pd.DataFrame(sentences, columns = columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8f22e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment(text):\n",
    "    scores = model.polarity_scores(text)\n",
    "    return scores.get('compound')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63629d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>positivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple cherche à acheter une start-up anglaise ...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Les voitures autonomes déplacent la responsabi...</td>\n",
       "      <td>0.1779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>San Francisco envisage d'interdire les robots ...</td>\n",
       "      <td>-0.0772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Londres est une grande ville du Royaume-Uni</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L’Italie choisit ArcelorMittal pour reprendre ...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Apple lance HomePod parce qu'il se sent menacé...</td>\n",
       "      <td>-0.0772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>La France ne devrait pas manquer d'électricité...</td>\n",
       "      <td>0.2411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nouvelles attaques de Trump contre le maire de...</td>\n",
       "      <td>-0.4767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Où es-tu ?</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Qui est le président de la France ?</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Où est la capitale des États-Unis ?</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Quand est né Barack Obama ?</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentences  positivity\n",
       "0   Apple cherche à acheter une start-up anglaise ...      0.0000\n",
       "1   Les voitures autonomes déplacent la responsabi...      0.1779\n",
       "2   San Francisco envisage d'interdire les robots ...     -0.0772\n",
       "3         Londres est une grande ville du Royaume-Uni      0.0000\n",
       "4   L’Italie choisit ArcelorMittal pour reprendre ...      0.0000\n",
       "5   Apple lance HomePod parce qu'il se sent menacé...     -0.0772\n",
       "6   La France ne devrait pas manquer d'électricité...      0.2411\n",
       "7   Nouvelles attaques de Trump contre le maire de...     -0.4767\n",
       "8                                          Où es-tu ?      0.0000\n",
       "9                 Qui est le président de la France ?      0.0000\n",
       "10                Où est la capitale des États-Unis ?      0.0000\n",
       "11                        Quand est né Barack Obama ?      0.0000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['positivity'] = np.vectorize(get_sentiment)(df.sentences)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c07fa330",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_lg')\n",
    "\n",
    "def get_entities(text):\n",
    "    doc = nlp(str(text))\n",
    "    list_ = []\n",
    "    for ent in doc.ents:\n",
    "        temp_ = re.sub('[^a-z0-9]+','_',ent.text.lower())\n",
    "        list_.append('__'.join([temp_, ent.label_.upper()]))\n",
    "    return ' '.join(list_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088817aa",
   "metadata": {},
   "source": [
    "df['entities'] = np.vectorize(get_entities)(df.sentences)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "340736df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nouns(text):\n",
    "    doc = nlp(str(text))\n",
    "    return [token for token in doc if token.pos_ == \"NOUN\"]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4d408a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[milliard, dollars]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nouns(str(sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "724f7e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject(text):\n",
    "    doc = nlp(str(text))\n",
    "    return [token for token in doc if token.dep_ == \"nsubj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "debb3c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Apple]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_subject(str(sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82b9879a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>positivity</th>\n",
       "      <th>nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple cherche à acheter une start-up anglaise ...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[milliard, dollars]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Les voitures autonomes déplacent la responsabi...</td>\n",
       "      <td>0.1779</td>\n",
       "      <td>[voitures, responsabilité, assurance, construc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>San Francisco envisage d'interdire les robots ...</td>\n",
       "      <td>-0.0772</td>\n",
       "      <td>[Francisco, robots, coursiers, trottoirs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Londres est une grande ville du Royaume-Uni</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[ville]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L’Italie choisit ArcelorMittal pour reprendre ...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[aciérie]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Apple lance HomePod parce qu'il se sent menacé...</td>\n",
       "      <td>-0.0772</td>\n",
       "      <td>[Echo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>La France ne devrait pas manquer d'électricité...</td>\n",
       "      <td>0.2411</td>\n",
       "      <td>[électricité, été, cas, canicule]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nouvelles attaques de Trump contre le maire de...</td>\n",
       "      <td>-0.4767</td>\n",
       "      <td>[attaques, maire]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Où es-tu ?</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Qui est le président de la France ?</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[président]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Où est la capitale des États-Unis ?</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[capitale]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Quand est né Barack Obama ?</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentences  positivity  \\\n",
       "0   Apple cherche à acheter une start-up anglaise ...      0.0000   \n",
       "1   Les voitures autonomes déplacent la responsabi...      0.1779   \n",
       "2   San Francisco envisage d'interdire les robots ...     -0.0772   \n",
       "3         Londres est une grande ville du Royaume-Uni      0.0000   \n",
       "4   L’Italie choisit ArcelorMittal pour reprendre ...      0.0000   \n",
       "5   Apple lance HomePod parce qu'il se sent menacé...     -0.0772   \n",
       "6   La France ne devrait pas manquer d'électricité...      0.2411   \n",
       "7   Nouvelles attaques de Trump contre le maire de...     -0.4767   \n",
       "8                                          Où es-tu ?      0.0000   \n",
       "9                 Qui est le président de la France ?      0.0000   \n",
       "10                Où est la capitale des États-Unis ?      0.0000   \n",
       "11                        Quand est né Barack Obama ?      0.0000   \n",
       "\n",
       "                                                nouns  \n",
       "0                                 [milliard, dollars]  \n",
       "1   [voitures, responsabilité, assurance, construc...  \n",
       "2           [Francisco, robots, coursiers, trottoirs]  \n",
       "3                                             [ville]  \n",
       "4                                           [aciérie]  \n",
       "5                                              [Echo]  \n",
       "6                   [électricité, été, cas, canicule]  \n",
       "7                                   [attaques, maire]  \n",
       "8                                                  []  \n",
       "9                                         [président]  \n",
       "10                                         [capitale]  \n",
       "11                                                 []  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['nouns'] = np.vectorize(get_nouns)(df.sentences)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94118b9a",
   "metadata": {},
   "source": [
    "**StopWords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e26fe210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['antérieures',\n",
       " 'néanmoins',\n",
       " 'tu',\n",
       " 'jusqu',\n",
       " 'facon',\n",
       " 'via',\n",
       " 'nous-mêmes',\n",
       " 'eh',\n",
       " 'suffit',\n",
       " 'd’']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_stopwords = list(spacy.lang.fr.stop_words.STOP_WORDS)\n",
    "[random.choice(spacy_stopwords) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d5d9f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def without_stopwords(text):\n",
    "    doc = nlp(str(text))\n",
    "    return [token for token in doc if not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f77a792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Apple, cherche, acheter, start, -, up, anglaise, 1, milliard, dollars]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "without_stopwords(str(sentences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3067e877",
   "metadata": {},
   "source": [
    "**Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fe230af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmatization(text):\n",
    "    doc = nlp(str(text))\n",
    "    return [\"{0} : {1}\".format(token, token.lemma_) for token in doc if str(token) != str(token.lemma_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "06697948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['une : un', 'anglaise : anglais', 'dollars : dollar']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lemmatization(str(sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6b9ef40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_frequency(text):\n",
    "    doc = nlp(str(text))\n",
    "    return [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "def display_frequency(words, size):\n",
    "    return Counter(words).most_common(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8837931a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('voiture', 1),\n",
       " ('autonome', 1),\n",
       " ('déplacer', 1),\n",
       " ('responsabilité', 1),\n",
       " ('assurance', 1)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_frequency(get_word_frequency(sentences[1]),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6162230c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('voiture', 1),\n",
       " ('autonome', 1),\n",
       " ('déplacer', 1),\n",
       " ('responsabilité', 1),\n",
       " ('assurance', 1)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_frequency(get_word_frequency(sentences[1]),5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}